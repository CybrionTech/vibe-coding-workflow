# Mission Order Standards (v8)

## Purpose
Mission Orders are lightweight but rigorous instructions for LLM agents to build and validate software in iterative versions. They are a contract between the project owner and agents, ensuring clarity, traceability, and alignment with the roadmap.

---

## File Naming & Location

Mission Orders and related documents are organized in version-specific folders:

```
docs/versions/vX.X/
├── mission-order.md
├── uat-automated.md
├── uat-user.md (if GUI in scope)
├── status-report.md
└── zen-plan.md (auto-generated by Claude)
```

**Naming Convention:**
- Mission Order: `mission-order.md` (within version folder)
- Automated UAT: `uat-automated.md`
- User UAT: `uat-user.md`
- Status Report: `status-report.md`
- Lessons Learned: `lessons-learned.md` (in `docs/` root, shared across versions)

---

## Structure of a Mission Order
Each Mission Order **must** include the following sections (in this order):

1. **Header**
   - Version (e.g., `v0.3`), Date, Author, Related roadmap entry.

2. **Objectives**
   - Concise statement of required outcomes for this version, mapped to the roadmap.

3. **Scope Definition**
   - List all applicable scopes explicitly (only what is relevant):
     - Backend / API
     - Frontend / GUI
     - CLI / Tooling
     - Infrastructure / Deployment
     - Documentation / Playbooks
     - Other (if applicable)

   **UAT Scope Rules**
   - If **only API/CLI are in scope**:
     - Agent must author and execute the automated UAT Playbook directly, capturing results inline.
   - If **GUI is in scope**:
     - UAT must be split into **two documents**:
       - **Automated UAT:** agent executes autonomously, records results (`uat-automated.md`)
       - **User UAT:** user executes; agent assists and records results (`uat-user.md`). Labeled **GUI Pending** until completed.

4. **In Scope**
   - Features and deliverables included in this version.

5. **Out of Scope**
   - Explicit items intentionally excluded for this version.

6. **Dependencies**
   - Prior versions that must be complete/approved
   - External prerequisites (datasets, credentials, configs)
   - Dependency management requirements (see technical-handbook.md section 4)

7. **Constraints**
   - Limitations that bind the work (e.g., "no external API calls," "local CSV only," performance caps).

8. **Environment Configuration** *(Required)*
   
   **Environment Types:**
   - Development (local machine)
   - Staging/Test (pre-production)
   - Production (live system)
   
   **Configuration Management:**
   - All environment-specific values externalized (`.env` files, environment variables)
   - No hardcoded credentials, API keys, or environment-specific URLs in code
   - `.env.example` provided with placeholder values
   - Secrets management strategy defined (where applicable)
   
   **Required for this version:**
   - List environment variables needed (e.g., `DATABASE_URL`, `API_KEY`)
   - Document default values for development
   - Specify production requirements
   - Migration strategy if changing configuration structure

9. **Execution Tasks**
   - Actionable, **imperative** steps aligned to PVDD (e.g., "Implement local provider to load OHLCV from `/data/seed/*.csv`").

10. **Definition of Done (DoD)**
    - Testable criteria. Each DoD item must map 1:1 to UAT steps.
    
    **Frontend Smoke Tests (Required when GUI in scope):**
    When scope includes "Frontend / GUI", the following items **must** be included as explicit DoD items:
    - DoD-X: All new routes return 200 (not 404)
    - DoD-Y: New UI components render in DOM
    - DoD-Z: Button click handlers registered
    - DoD-N: No console errors on page load

   ## Testing Requirements

   All versions must include automated tests that prove DoD completion.

   ### Test Types Required
   - **Unit Tests:** Test individual functions/classes in isolation
   - **Integration Tests:** Test actual code against actual dependencies
   - **DoD Validation Tests:** At least one test per DoD item that proves compliance

   ### Test Execution
   - Tests must import actual source code (not mocks)
   - Tests must run against implemented code
   - Tests must fail before implementation (Red phase)
   - Tests must pass after implementation (Green phase)

   ### Coverage Requirements
   - Minimum 80% code coverage required for DoD completion
   - Coverage report must be generated and included in Status Report
   - Uncovered code must be justified (e.g., platform-specific stubs)

   ### DoD-to-Test Mapping
   Each DoD item must list:
   - Which test file proves it
   - Which test function(s) validate it
   - What the test asserts

   **Example:**
   DoD-1: Config loads from ~/.config/netmon/config.yaml
   Evidence: tests/test_config.py::test_config_loads_from_xdg_path (line 45)

11. **Git Commit Directives**
    - **Commit Strategy:** [single | per-objective | milestone]
    - **Branch Pattern:** `feature/{version}/{brief-name}`
    - **Message Format:** `[{version}] {type}: {description}`
    - **Message Type:** Use conventional commits (see technical-handbook.md section 3)
      - `feat`: New feature
      - `fix`: Bug fix
      - `docs`: Documentation only
      - `test`: Adding/updating tests
      - `refactor`: Code restructure (no behavior change)
      - `perf`: Performance improvement
      - `chore`: Maintenance (dependencies, configs)
      - `security`: Security-related changes
    
    **Commit Timing:**
    - Mission Order: When user passes to agent (before implementation)
    - UAT playbooks: When user adds approval notes (complete validation record)
    - Status Report & Lessons Learned: When version marked complete
    
    **Auto-commit conditions:**
    - Automated UAT section passes
    - Tests passing
    - Code review complete (if applicable)

12. **Deliverables**
    - Code, configs, docs.
    - **UAT Playbooks:** 
      - `uat-automated.md` (authored and executed per Scope Definition rules)
      - `uat-user.md` (if GUI in scope)
    - **Status Report:** created **only after** UAT approval.
    - **Lessons Learned:** updated **only after** version marked complete.
    - **Zen Plan:** auto-generated by Claude during implementation (not manually created)
    - **Environment Configuration:** `.env.example` and configuration documentation

---

## UAT Playbook Versioning

If substantial changes are needed to UAT playbooks during execution:

- **Initial version:** `uat-automated.md` or `uat-user.md`
- **Major revision needed:** Create `uat-automated-v2.md` in same version folder
- **Mark superseded:** Add note to original: "⚠️ SUPERSEDED by uat-automated-v2.md"

**Preserve all versions** - move old versions to `docs/archive/` when superseded, don't delete.

---

## Collaboration, Zen Planning & Traceability

### Hive Agents (required)
- All mission orders must be executed within a hive-mind context.
- Spawned agents must coordinate and collaborate with each other throughout the PVDD loop.
- Mission Orders must explicitly instruct agents to operate in a hive configuration, ensuring peer review and shared responsibility rather than isolated execution.
- ChatGPT collaboration remains mandatory for plan validation and end-of-cycle review.

### Zen Planning (required)
- Every Mission Order begins with a **Zen Plan** produced by the agent before coding:
  - Scope confirmation, execution sequencing, risk/assumption log.
  - Cross-check against Objectives, Scope Definition, Constraints, and DoD.
- The Zen Plan is **auto-generated by Claude and saved to `docs/versions/vX.X/zen-plan.md`**
- The Zen Plan **must be reviewed by ChatGPT** (via Zen MCP) **before implementation**.

### ChatGPT Collaboration
- **Mission Orders must be passed into ChatGPT at the beginning and the end of each cycle.**
  - **Beginning:** ChatGPT validates the Zen Plan, Dependencies, Constraints, and DoD.
  - **End (before user UAT):** ChatGPT validates completed code for:
    - Router registration in backend
    - Component imports in frontend
    - Event handler wiring
    - Common integration mistakes
    - Environment configuration (no hardcoded secrets)
  - **End (after UAT):** ChatGPT reviews implementation, UAT outcomes, Status Report, and Lessons Learned for completeness/consistency.
- Claude and ChatGPT collaborate continuously via Zen MCP; ChatGPT acts as reviewer/validator.

### Traceability Rules
- Mission Order must reference the exact roadmap entry **and explicitly call out to review the `roadmap.md` file**.
- UAT steps must map directly to DoD items (one-to-one).
- Status Report must link outcomes back to Objectives and UAT results.
- Lessons Learned must record gaps and applied fixes that inform future Mission Orders.

---

## PVDD Loop (enforced)

- **Plan:** Draft Zen Plan (auto-generated), confirm scope/deps/constraints; share Mission Order + Zen Plan with ChatGPT.

- **Verify:** Validate design/approach against Mission Order and DoD; adjust before coding.

- **Develop:** Implement features, tests, and docs as per Execution Tasks.
  - Write tests first (TDD: Red phase - tests fail)
  - Implement code to pass tests (Green phase)
  - Run tests against implementation
  - Generate coverage report (minimum 80%)

- **Validate:** Integration validation before UAT
  - Run all tests against implemented code
  - Verify coverage meets minimum (80%)
  - Map each DoD item to passing test (include in Status Report)
  - ChatGPT validates completed code:
    - Router registration in backend
    - Component imports in frontend
    - Event handler wiring
    - Environment configuration (no hardcoded secrets)
    - Common integration mistakes
  - Fix any issues before proceeding to Deploy

- **Deploy:** Run automated UAT, obtain user approval on user UAT (if applicable), publish Status Report, and finalize Lessons Learned.

---

## Authoring Notes
- Write in clear, imperative voice; avoid ambiguity.
- Keep UAT Playbooks focused: automated UAT tests integration, user UAT validates experience.
- Prefer deterministic, reproducible datasets and golden tests.
- Explicitly flag anything deferred to a later version.
- Follow efficiency patterns: batch operations, concurrent execution, proper file organization.
- Ensure environment configuration externalized (no secrets in code).
- Document dependency changes and security audits.
